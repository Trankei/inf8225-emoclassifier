{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage import io, transform\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import warnings\n",
    "\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications.inception_v3 import preprocess_input, decode_predictions, InceptionV3\n",
    "from keras.models import model_from_json, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditDataset(Dataset):\n",
    "    \"\"\"Reddit dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, folder):\n",
    "        \"\"\"\n",
    "        Arg:\n",
    "            folder(string): Path to the folder that contains the csv files.\n",
    "        \"\"\"\n",
    "        happy_data = pd.read_csv(folder + '/processed_happy.csv')\n",
    "        creepy_data = pd.read_csv(folder + '/processed_creepy.csv')\n",
    "        gore_data = pd.read_csv(folder + '/processed_gore.csv')\n",
    "        rage_data = pd.read_csv(folder +'/processed_rage.csv')\n",
    "        self.reddit_data = (pd.concat([happy_data, creepy_data, gore_data,rage_data])).sort_values(by = 'ups', ascending=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reddit_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_url = self.reddit_data.iloc[idx, 2]\n",
    "        if 'imgur' in img_url:\n",
    "            if '.jpeg' not in img_url :\n",
    "                if '.jpg'not in img_url : \n",
    "                    if '.png'not in img_url : \n",
    "                        requete = requests.get(img_url)\n",
    "                        page = requete.content\n",
    "                        soup = BeautifulSoup(page)\n",
    "                        img_url = soup.find('link', rel=\"image_src\", href=True)\n",
    "                        if (img_url): \n",
    "                            img_url = img_url['href']\n",
    "        img_title = self.reddit_data.iloc[idx, 6]\n",
    "        img_label = self.reddit_data.iloc[idx, 8]\n",
    "        \n",
    "        try:\n",
    "            image = io.imread(img_url)   \n",
    "        except:\n",
    "            print(\"Requested photo not available.\")\n",
    "            return {}\n",
    "        else:\n",
    "            try:\n",
    "                image = transform.resize(image, (299, 299, 3))\n",
    "            except:\n",
    "                print(\"Image dimensions are too small\")\n",
    "                return {}\n",
    "            else :\n",
    "                if image.ndim == 3 :\n",
    "                    sample = {'image': image,'description': img_title, 'label': img_label}\n",
    "                    return(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    batch = list(filter (lambda x:x is not None, batch))\n",
    "    return default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and dataloader\n",
    "reddit_dataset = RedditDataset('data/reddit_data')\n",
    "loader = DataLoader(reddit_dataset, batch_size=1, shuffle=True, collate_fn=my_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded image feature extractor\n"
     ]
    }
   ],
   "source": [
    "# Once model is loaded and saved you can load it everytime you need it\n",
    "# load json and create model\n",
    "json_file = open(\"models/inception_feature_extractor.json\", 'r')\n",
    "inception_feature_extractor = json_file.read()\n",
    "json_file.close()\n",
    "inception_feature_extractor = model_from_json(inception_feature_extractor)\n",
    "# load weights into new model\n",
    "inception_feature_extractor.load_weights(\"models/inception_feature_extractor.h5\")\n",
    "print(\"Loaded image feature extractor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Image dimensions are too small\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Image dimensions are too small\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Image dimensions are too small\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Image dimensions are too small\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Image dimensions are too small\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Image dimensions are too small\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Image dimensions are too small\n",
      "Requested photo not available.\n",
      "Image dimensions are too small\n",
      "Requested photo not available.\n",
      "Image dimensions are too small\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Image dimensions are too small\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Image dimensions are too small\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "Requested photo not available.\n",
      "(3193, 2048)\n",
      "(3193, 1)\n",
      "(3193, 1)\n"
     ]
    }
   ],
   "source": [
    "image_features = []\n",
    "text = []\n",
    "labels = []\n",
    "for sample in loader :\n",
    "    if sample:\n",
    "        np_image = sample['image'].numpy()\n",
    "        image_vect = inception_feature_extractor.predict(np_image)\n",
    "        image_features.append(image_vect)\n",
    "        labels.append(sample['label'])\n",
    "        text.append(sample['description'])\n",
    "image_features = np.array(image_features).squeeze()\n",
    "text = np.array(text)\n",
    "labels = np.array(labels)\n",
    "print(image_features.shape)\n",
    "print(text.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features_train, image_features_test, labels_train, labels_test = train_test_split(image_features, labels, test_size=0.20, random_state=42)\n",
    "text_train, text_test, labels_train, labels_test = train_test_split(text, labels, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(image_features_test, open(f'processed_data/image_testing_features.pkl', 'wb'))\n",
    "pickle.dump(text_test, open(f'processed_data/text_testing.pkl', 'wb'))\n",
    "# np.savetxt(\"processed_data/text_testing_features.csv\", test_text, header='title', fmt='%s', comments='')\n",
    "pickle.dump(labels_test, open(f'processed_data/testing_labels.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(valid_image_features, open(f'processed_data/image_validation_features.pkl', 'wb'))\n",
    "# pickle.dump(valid_text, open(f'processed_data/text_validation.pkl', 'wb'))\n",
    "# # np.savetxt(\"processed_data/text_validation_features.csv\", valid_text, header='title', fmt='%s', comments='')\n",
    "# pickle.dump(valid_labels, open(f'processed_data/validation_labels.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(image_features_train, open(f'processed_data/image_training_features.pkl', 'wb'))\n",
    "pickle.dump(text_train, open(f'processed_data/text_training.pkl', 'wb'))\n",
    "# np.savetxt(\"processed_data/text_training_features.csv\", train_text, header='title', fmt='%s', comments='')\n",
    "pickle.dump(labels_train, open(f'processed_data/training_labels.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
